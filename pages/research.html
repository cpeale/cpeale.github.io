<!DOCTYPE html>
<!-- saved from url=(0036)https://khwilson.github.io/TestRepo/ -->
<html><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <meta name="viewport" content="width=device-width, initial-scale=1, minimal-ui">
    <title>Charlotte Peale</title>
    <link rel="stylesheet" href="../index_files/github-markdown.css">
    <style>
      body {
        box-sizing: border-box;
        min-width: 200px;
        max-width: 980px;
        margin: 0 auto;
        padding: 45px;
      }
    </style>
  </head>
  <body><panel class="drweb_select-panel" style="display: none;">
  <div class="drweb_tool-panel">
    <div class="drweb_tool-icon drweb_tool-icon_show" data-co="restore-btn" title="restore"></div>
    <div class="drweb_tool-icon drweb_tool-icon_remove" data-co="remove-btn" title="remove"></div>
    <div class="drweb_fit-btn" data-co="fit-btn" title=""></div>
  </div>
  <span data-co="label">Select the elements you want to hide on this page.</span>
  <div class="drweb_btn" data-co="save">Save</div>
  <div class="drweb_btn" data-co="cancel">Cancel</div>
  <div class="drweb_panel-plug" data-co="plug" style="display: none"></div>
</panel>
    <article class="markdown-body">
      <p align="center">
        <a href='../index.html'>
          <img src='../index_files/name.PNG'>
        </a>
      </p>
      <h2>Research</h2>
      <p>I want to ensure that theoretical computer science considers and
        protects the perspectives of groups whose interests it has historically
        overlooked.</p>
        <p>My research focuses on how we can protect
        the rights of individuals both at the high-level of defining frameworks
        to analyze how algorithms adhere to societal values (i.e. privacy,
        fairness), and at a lower level of explicitly designing systems that
        have built-in abuse mitigation and other protections rather than
        depending on policies alone to ensure ethical behavior.</p>
        
      <h4> Note: Unfortunately this page is not fully up-to-date with all the exciting things I've been working on! Please check back soon for more recent work.<\h4>

      <h3> Algorithmic Fairness and Diversity </h3>

      <p>Since September 2020, I have been working with Omer Reingold and
      Judy Shen on questions related to how to analyze and design algorithms that
      encourage diversity.</p>

      <h3> Secure Source-Tracking for Encrypted Messaging Systems </h3>

      <p>End-to-end encrypted messaging systems such as WhatsApp are
      attractive to users because they provide strong privacy and security
      guarantees. These platforms, however, have become victims of viral
      forwards of misinformation and illegal content, facilitated by their
      anonymous guarantees. Without knowing who is sending these problematic
      messages, secure messaging platforms are unable to enforce the same type
      of content moderation employed by sites such as Twitter to prevent the
      viral spread of misinformation via forwards. </p>

      <p>I spent summer 2020 working on a project in applied cryptography
      under the mentorship of Dan Boneh and Saba Eskandarian. Our work
      developed tools to control the spread
      of disinformation and malicious content in end-to-end encrypted messaging
      systems, and was awarded a CURIS Outstanding Poster Award. </p>

      <p><a href="../index_files/Peale_CURIS_Poster.png">Poster (CURIS 2020)</a></p>

      <h3> Bounded-Leakage Differential Privacy </h3>

      <p> How is an individual's privacy affected when a differentially private
        release of data is combined with an existing background of auxiliary information?</p>

      <p> To help answer this question, I worked with Omer Reingold and Katrina
        Ligett starting in Fall 2019 to define a new variant of differential privacy: bounded-leakage
         differential privacy, that gives a tighter measure of privacy with respect
         to some upper bound on potential auxiliary information (leakage).</p>
      <p>This new definition can be applied to bring new insights
         to settings such as releasing exact counts of census data, or reasoning about
         whether your privacy can be affected by studies that you did not participate in.
       </p>

       <p>I presented our work at the 2020 Symposium on the Foundations of
         Responsible Computing in June, and it was featured as a poster at TPDP
         2020. We are currently working on a longer version of the paper for
         broader publication.  </p>

         <p><a href="../index_files/TPDP_Poster_Final.pdf">Poster (TPDP 2020)</a> |
           <a href="https://www.youtube.com/watch?v=ErhNeQArYzY&ab_channel=FORC2020">Presentation (FORC 2020)</a> |
           <a href="https://drops.dagstuhl.de/opus/volltexte/2020/12026/pdf/LIPIcs-FORC-2020-10.pdf">Paper (FORC 2020)</a></p>
    </article>


</body></html>
